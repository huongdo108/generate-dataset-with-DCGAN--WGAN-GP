# Generate dataset with Generative adversarial networks (GANs): Deep Convolutional GAN (DCGAN) and Wasserstein GAN with gradient penalty (WGAN-GP)



## Overview
The goal of this repository is to get familiar with generative adversarial networks and specifically DCGAN and WGAN-GP. GANs are models from which we can draw samples that will have a distribution similar to the distribution of the training data.


DCGAN was proposed by [Radford et al., 2015](https://arxiv.org/pdf/1511.06434.pdf).. WGAN-GP was proposed by  [Improved Training of Wasserstein GANs](https://arxiv.org/pdf/1704.00028.pdf).

## Dataset 
MNIST digits data from torchvision.datasets

## Deep Convolutional GAN (DCGAN)
DCGAN architecture includes 2 main components: Generator and Discriminator

**Generator**

The generative model that I use is:

<img src="https://github.com/huongdo108/generate-dataset-with-DCGAN--WGAN-GP/blob/master/images/dcgan_generator.PNG" align="centre">



The data is generated by applying a nonlinear transformation to samples drawn from the standard normal distribution.

**G** is modeled with a deep neural network. In DCGAN, the generator is made of only transposed convolutional layers `ConvTranspose2d` followed by `tanh`. 
The `tanh` nonlinearity guarantees that the output is between -1 and 1 which holds for our scaling of the training data.

**Loss for training the generator**

The generative model will be guided by a discriminator whose task is to separate (classify) data into two classes:
* true data (samples from the training set)
* generated data (samples generated by the generator).

The task of the generator is to confuse the discriminator as much as possible, which is the case when the distribution produced by the generator perfectly replicates the data distribution. Thus, a loss function is implemented to train the generator. The loss is the `binary_cross_entropy` loss computed with `real_label` as targets for the generated samples.

**Discriminator**

In DCGAN, the discriminator is a stack of only convolutional layers.

**Loss for training the discriminator**

The discriminator is trained to solve a binary classification problem: to separate real data from generated samples. Thus, the output of the discriminator should be a scalar between 0 and 1. 

A loss function is implemented to train the discriminator. The dicriminator uses the `binary_cross_entropy` loss,  `real_label` as targets for real samples and `fake_label` as targets for generated samples.

**Evaluate generated samples**

<img src="https://github.com/huongdo108/generate-dataset-with-DCGAN--WGAN-GP/blob/master/images/dcgan_score.PNG" align="centre">

## Wasserstein GAN with gradient penalty (WGAN-GP)

WGAN-GP architecture includes 2 main components: Generator and Critic

The WGAN value function is constructed as

<img src="https://github.com/huongdo108/generate-dataset-with-DCGAN--WGAN-GP/blob/master/images/wgan.PNG" align="centre">

where
* the dicriminator <img src="https://render.githubusercontent.com/render/math?math=D">**** (called critic in WGAN) is constrained to be from the set <img src="https://render.githubusercontent.com/render/math?math=\mathcal{D}"> of 1-Lipschitz functions
* <img src="https://render.githubusercontent.com/render/math?math=P_r"> is the data distribution
* <img src="https://render.githubusercontent.com/render/math?math=P_g"> is the model distribution. Samples from the model distribution are produced as follows:

<img src="https://render.githubusercontent.com/render/math?math=z \sim N(0, I)">

and

<img src="https://render.githubusercontent.com/render/math?math=\tilde x = G(z)">

**Generator**

The same architecture with DCGAN's generator

**Loss for training the generator**

The generator is trained to minimize the relevant part of the value function using a fixed critic <img src="https://render.githubusercontent.com/render/math?math=D">:


<img src="https://github.com/huongdo108/generate-dataset-with-DCGAN--WGAN-GP/blob/master/images/wgan_g_loss.PNG" align="centre">

**Critic**

In WGAN-GP, the discriminator is called a critic because it is not trained to classify. 

**Loss for training the WGAN critic**

Recall the value function of WGAN:

<img src="https://github.com/huongdo108/generate-dataset-with-DCGAN--WGAN-GP/blob/master/images/wgan_critic1.PNG" align="centre">

To tune the critic, the following function needs to be minimized assuming no constraints on <img src="https://render.githubusercontent.com/render/math?math=D">

<img src="https://github.com/huongdo108/generate-dataset-with-DCGAN--WGAN-GP/blob/master/images/wgan_critic2.PNG" align="centre">



**Gradient penalty**

Without constraints on <img src="https://render.githubusercontent.com/render/math?math=D">, the WGAN value function can be made infinitely large. WGAN constrains the derivative of <img src="https://render.githubusercontent.com/render/math?math=D"> using a gradient penalty. The penalty is computed at random points between real images and generated ones using the following procedure:
* Given a real image **x** and a fake image <img src="https://render.githubusercontent.com/render/math?math=\tilde x">, draw a random number <img src="https://render.githubusercontent.com/render/math?math=\epsilon \sim U[0,1]">
* <img src="https://github.com/huongdo108/generate-dataset-with-DCGAN--WGAN-GP/blob/master/images/gradient_penalty2.PNG" align="centre">
* Compute the gradient penalty <img src="https://github.com/huongdo108/generate-dataset-with-DCGAN--WGAN-GP/blob/master/images/gradient_penalty.PNG" align="centre">
where <img src="https://render.githubusercontent.com/render/math?math=\nabla_{\hat{x}} D(\hat{x})"> is the gradient of <img src="https://render.githubusercontent.com/render/math?math=D"> computed at <img src="https://render.githubusercontent.com/render/math?math=\hat{x}">.

**Evaluate generated samples**

<img src="https://github.com/huongdo108/generate-dataset-with-DCGAN--WGAN-GP/blob/master/images/wgan_score.PNG" align="centre">
